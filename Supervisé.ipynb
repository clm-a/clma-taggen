{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fo8jsJQVnc7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 14:35:00.028767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730813700.122664  203716 cuda_dnn.cc:8498] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730813700.149481  203716 cuda_blas.cc:1410] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 14:35:00.375096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR']=\"cuda_malloc_async\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1qGqFRXnc7e",
    "outputId": "54388006-2e32-46a4-b758-b128672b57c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0-dev20241104\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A27rfjHlnc7e"
   },
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "data = pd.read_csv('data/monthly_best_1000_cleaned.csv', sep=\";\")\n",
    "\n",
    "data = data.sample(50000)\n",
    "\n",
    "# Suppression des lignes avec des valeurs manquantes\n",
    "data.dropna(subset=['Body', 'Tags'], inplace=True)\n",
    "\n",
    "# Conversion des tags en listes\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Filtrage des tags rares\n",
    "from collections import Counter\n",
    "tag_counts = Counter(tag for tags in data['Tags'] for tag in tags)\n",
    "min_tag_frequency = 50\n",
    "frequent_tags = {tag for tag, count in tag_counts.items() if count >= min_tag_frequency}\n",
    "data['Tags'] = data['Tags'].apply(lambda tags: [tag for tag in tags if tag in frequent_tags])\n",
    "data = data[data['Tags'].map(len) > 0]\n",
    "\n",
    "# Binarisation des tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(data['Tags'])\n",
    "\n",
    "# S√©paration en ensembles d'entra√Ænement et de test\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    data['Body'], y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOLOVqv8nc7f",
    "outputId": "583165f6-31de-4713-df76-2ede7a23bb4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/clement/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Fonction de pr√©traitement\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Application du pr√©traitement\n",
    "X_train_tokens = X_train_text.apply(preprocess_text)\n",
    "X_test_tokens = X_test_text.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KgQOsuWnc7f"
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TgQUpWQznc7f"
   },
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le Word2Vec\n",
    "w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Fonction pour obtenir l'embedding moyen d'un texte\n",
    "def get_w2v_embedding(tokens):\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(w2v_model.vector_size)\n",
    "\n",
    "# Obtention des embeddings pour les ensembles d'entra√Ænement et de test\n",
    "X_train_w2v = np.array([get_w2v_embedding(tokens) for tokens in X_train_tokens])\n",
    "X_test_w2v = np.array([get_w2v_embedding(tokens) for tokens in X_test_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hS5tHcErnc7f",
    "outputId": "ca256f11-21a3-4e59-d358-46d617d697a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/05 14:59:27 INFO mlflow.tracking.fluent: Experiment with name 'Word2Vec' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow tracking uri: http://0.0.0.0:5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/05 15:00:46 INFO mlflow.tracking._tracking_service.client: üèÉ View run rambunctious-dog-241 at: http://0.0.0.0:5000/#/experiments/837592451353230536/runs/2afca1f7cb184626a05b410c2610922d.\n",
      "2024/11/05 15:00:46 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://0.0.0.0:5000/#/experiments/837592451353230536.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) avec Word2Vec : 0.4282\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Cr√©ation du mod√®le de r√©gression logistique\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Comme nous avons un probl√®me de classification multiclasse multilabel, nous utilisons OneVsRestClassifier\n",
    "log_reg_w2v = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    "\n",
    "mlflow.set_tracking_uri('http://0.0.0.0:5000')\n",
    "experiment = mlflow.set_experiment(\"Word2Vec\")\n",
    "print(\"mlflow tracking uri:\", mlflow.tracking.get_tracking_uri())\n",
    "\n",
    "\n",
    "with  mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "\n",
    "  # Entra√Ænement du mod√®le\n",
    "  log_reg_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "  # Pr√©dictions\n",
    "  y_pred_w2v = log_reg_w2v.predict(X_test_w2v)\n",
    "\n",
    "  # √âvaluation\n",
    "  f1_w2v = f1_score(y_test, y_pred_w2v, average='micro')\n",
    "  print(f\"Score F1 (micro) avec Word2Vec : {f1_w2v:.4f}\")\n",
    "  \n",
    "  mlflow.log_param(\"f1 micro\", f1_w2v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjBp9nKlnc7f"
   },
   "source": [
    "## Exemple de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jzBgWkmnc7g",
    "outputId": "eb334269-ac32-4f9e-f95c-96f00c0d0e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte d'exemple : How can I implement a neural network in Python?\n",
      "\n",
      "Tags les plus probables (Word2Vec) :\n",
      "- python: 0.9999\n",
      "- tensorflow: 0.9885\n",
      "- keras: 0.9506\n",
      "- deep-learning: 0.9247\n",
      "- kotlin-coroutines: 0.8944\n"
     ]
    }
   ],
   "source": [
    "# Texte d'exemple\n",
    "example_text = \"How can I implement a neural network in Python?\"\n",
    "\n",
    "# Pr√©traitement\n",
    "example_tokens = preprocess_text(example_text)\n",
    "\n",
    "# Obtention de l'embedding\n",
    "example_embedding = get_w2v_embedding(example_tokens).reshape(1, -1)\n",
    "\n",
    "# Pr√©diction des probabilit√©s\n",
    "y_prob_w2v = log_reg_w2v.predict_proba(example_embedding)[0]\n",
    "\n",
    "# Obtenir les tags avec les probabilit√©s les plus √©lev√©es\n",
    "top_n = 5  # Nombre de tags √† afficher\n",
    "top_indices = y_prob_w2v.argsort()[-top_n:][::-1]\n",
    "top_tags = mlb.classes_[top_indices]\n",
    "top_probs = y_prob_w2v[top_indices]\n",
    "\n",
    "# Affichage des tags les plus pertinents\n",
    "print(\"Texte d'exemple :\", example_text)\n",
    "print(\"\\nTags les plus probables (Word2Vec) :\")\n",
    "for tag, prob in zip(top_tags, top_probs):\n",
    "    print(f\"- {tag}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zoko2rx2nc7g"
   },
   "source": [
    "# Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Lq9aGJnbnc7g"
   },
   "outputs": [],
   "source": [
    "# Chargement du mod√®le USE\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Fonction pour obtenir les embeddings USE\n",
    "def get_use_embeddings(texts):\n",
    "    return use_model(texts).numpy()\n",
    "\n",
    "# Obtention des embeddings\n",
    "X_train_use = get_use_embeddings(X_train_text.tolist())\n",
    "X_test_use = get_use_embeddings(X_test_text.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hH4nX_Oknc7g",
    "outputId": "f5a91db3-e528-41f1-d539-165e219cd412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) avec USE : 0.3527\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation du mod√®le de r√©gression logistique\n",
    "log_reg_use = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    "\n",
    "# Entra√Ænement du mod√®le\n",
    "log_reg_use.fit(X_train_use, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_use = log_reg_use.predict(X_test_use)\n",
    "\n",
    "# √âvaluation\n",
    "f1_use = f1_score(y_test, y_pred_use, average='micro')\n",
    "print(f\"Score F1 (micro) avec USE : {f1_use:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcTys4Rwnc7g"
   },
   "source": [
    "## Exemple de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtLYA1AQnc7g",
    "outputId": "7b16e920-3372-4658-b890-d73a6027a8a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tags les plus probables (USE) :\n",
      "- python: 0.9833\n",
      "- python-3.x: 0.0762\n",
      "- c++: 0.0335\n",
      "- github: 0.0319\n",
      "- amazon-web-services: 0.0257\n"
     ]
    }
   ],
   "source": [
    "# Texte d'exemple\n",
    "example_text = \"How can I implement a neural network in Python?\"\n",
    "\n",
    "# Obtention de l'embedding\n",
    "example_embedding_use = get_use_embeddings([example_text])\n",
    "\n",
    "# Pr√©diction des probabilit√©s\n",
    "y_prob_use = log_reg_use.predict_proba(example_embedding_use)[0]\n",
    "\n",
    "# Obtenir les tags avec les probabilit√©s les plus √©lev√©es\n",
    "top_n = 5  # Nombre de tags √† afficher\n",
    "top_indices = y_prob_use.argsort()[-top_n:][::-1]\n",
    "top_tags = mlb.classes_[top_indices]\n",
    "top_probs = y_prob_use[top_indices]\n",
    "\n",
    "# Affichage des tags les plus pertinents\n",
    "print(\"\\nTags les plus probables (USE) :\")\n",
    "for tag, prob in zip(top_tags, top_probs):\n",
    "    print(f\"- {tag}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HV9el5Ync7g"
   },
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntBrp_2tnc7g",
    "outputId": "8472550e-2c42-4e38-a8a0-406d711e16a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Chargement du tokenizer et du mod√®le BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fonction pour obtenir les embeddings BERT\n",
    "def get_bert_embeddings(texts):\n",
    "    inputs = bert_tokenizer(texts, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
    "    outputs = bert_model(inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Obtention des embeddings\n",
    "X_train_bert = get_bert_embeddings(X_train_text.to_list())\n",
    "X_test_bert = get_bert_embeddings(X_test_text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTb7FoqJnc7h",
    "outputId": "9d42cce1-edcd-44d0-95e4-5048790c2d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F1 (micro) avec BERT : 0.4115\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation du mod√®le de r√©gression logistique\n",
    "log_reg_bert = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    "\n",
    "# Entra√Ænement du mod√®le\n",
    "log_reg_bert.fit(X_train_bert, y_train)\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_bert = log_reg_bert.predict(X_test_bert)\n",
    "\n",
    "# √âvaluation\n",
    "f1_bert = f1_score(y_test, y_pred_bert, average='micro')\n",
    "print(f\"Score F1 (micro) avec BERT : {f1_bert:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGq9visJnc7h"
   },
   "source": [
    "## Exemple de pr√©diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMuWQsWNnc7h",
    "outputId": "968ea036-9cf8-43ed-b04c-d331d1dfe429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tags les plus probables (BERT) :\n",
      "- python: 0.9280\n",
      "- python-3.x: 0.3438\n",
      "- c: 0.1322\n",
      "- docker: 0.0907\n",
      "- kotlin: 0.0778\n"
     ]
    }
   ],
   "source": [
    "# Texte d'exemple\n",
    "example_text = \"How can I implement a neural network in Python?\"\n",
    "\n",
    "# Obtention de l'embedding\n",
    "# Pass the example text as a single-element list\n",
    "example_embedding_bert = get_bert_embeddings([example_text])\n",
    "\n",
    "# Pr√©diction des probabilit√©s\n",
    "y_prob_bert = log_reg_bert.predict_proba(example_embedding_bert)[0]\n",
    "\n",
    "# Obtenir les tags avec les probabilit√©s les plus √©lev√©es\n",
    "top_n = 5  # Nombre de tags √† afficher\n",
    "top_indices = y_prob_bert.argsort()[-top_n:][::-1]\n",
    "top_tags = mlb.classes_[top_indices]\n",
    "top_probs = y_prob_bert[top_indices]\n",
    "\n",
    "# Affichage des tags les plus pertinents\n",
    "print(\"\\nTags les plus probables (BERT) :\")\n",
    "for tag, prob in zip(top_tags, top_probs):\n",
    "    print(f\"- {tag}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlJLLZnqnc7h"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0EC34o7nc7h",
    "outputId": "fab99b49-b607-4646-dd15-322b651e1244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores F1 (micro) :\n",
      "- Word2Vec : 0.1480\n",
      "- USE      : 0.3527\n",
      "- BERT     : 0.4115\n"
     ]
    }
   ],
   "source": [
    "# Affichage des scores F1\n",
    "print(\"Scores F1 (micro) :\")\n",
    "print(f\"- Word2Vec : {f1_w2v:.4f}\")\n",
    "print(f\"- USE      : {f1_use:.4f}\")\n",
    "print(f\"- BERT     : {f1_bert:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".sup_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
